{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b176722-4b2c-4b24-b16a-2c706a4f59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ec4abdf-d652-491e-a35a-910871424a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4a96f74-891f-4238-ae46-92c8a6d7445f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_source</th>\n",
       "      <th>industry</th>\n",
       "      <th>number_of_courses_viewed</th>\n",
       "      <th>annual_income</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>location</th>\n",
       "      <th>interaction_count</th>\n",
       "      <th>lead_score</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>79450.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>4</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>social_media</td>\n",
       "      <td>retail</td>\n",
       "      <td>1</td>\n",
       "      <td>46992.0</td>\n",
       "      <td>employed</td>\n",
       "      <td>south_america</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>events</td>\n",
       "      <td>healthcare</td>\n",
       "      <td>5</td>\n",
       "      <td>78796.0</td>\n",
       "      <td>unemployed</td>\n",
       "      <td>australia</td>\n",
       "      <td>3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>retail</td>\n",
       "      <td>2</td>\n",
       "      <td>83843.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>australia</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>referral</td>\n",
       "      <td>education</td>\n",
       "      <td>3</td>\n",
       "      <td>85012.0</td>\n",
       "      <td>self_employed</td>\n",
       "      <td>europe</td>\n",
       "      <td>3</td>\n",
       "      <td>0.62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>referral</td>\n",
       "      <td>manufacturing</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>self_employed</td>\n",
       "      <td>north_america</td>\n",
       "      <td>4</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>referral</td>\n",
       "      <td>technology</td>\n",
       "      <td>3</td>\n",
       "      <td>65259.0</td>\n",
       "      <td>student</td>\n",
       "      <td>europe</td>\n",
       "      <td>2</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>paid_ads</td>\n",
       "      <td>technology</td>\n",
       "      <td>1</td>\n",
       "      <td>45688.0</td>\n",
       "      <td>student</td>\n",
       "      <td>north_america</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>referral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>71016.0</td>\n",
       "      <td>self_employed</td>\n",
       "      <td>north_america</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>organic_search</td>\n",
       "      <td>finance</td>\n",
       "      <td>3</td>\n",
       "      <td>92855.0</td>\n",
       "      <td>student</td>\n",
       "      <td>north_america</td>\n",
       "      <td>3</td>\n",
       "      <td>0.41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1462 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         lead_source       industry  number_of_courses_viewed  annual_income  \\\n",
       "0           paid_ads            NaN                         1        79450.0   \n",
       "1       social_media         retail                         1        46992.0   \n",
       "2             events     healthcare                         5        78796.0   \n",
       "3           paid_ads         retail                         2        83843.0   \n",
       "4           referral      education                         3        85012.0   \n",
       "...              ...            ...                       ...            ...   \n",
       "1457        referral  manufacturing                         1            NaN   \n",
       "1458        referral     technology                         3        65259.0   \n",
       "1459        paid_ads     technology                         1        45688.0   \n",
       "1460        referral            NaN                         5        71016.0   \n",
       "1461  organic_search        finance                         3        92855.0   \n",
       "\n",
       "     employment_status       location  interaction_count  lead_score  \\\n",
       "0           unemployed  south_america                  4        0.94   \n",
       "1             employed  south_america                  1        0.80   \n",
       "2           unemployed      australia                  3        0.69   \n",
       "3                  NaN      australia                  1        0.87   \n",
       "4        self_employed         europe                  3        0.62   \n",
       "...                ...            ...                ...         ...   \n",
       "1457     self_employed  north_america                  4        0.53   \n",
       "1458           student         europe                  2        0.24   \n",
       "1459           student  north_america                  3        0.02   \n",
       "1460     self_employed  north_america                  0        0.25   \n",
       "1461           student  north_america                  3        0.41   \n",
       "\n",
       "      converted  \n",
       "0             1  \n",
       "1             0  \n",
       "2             1  \n",
       "3             0  \n",
       "4             1  \n",
       "...         ...  \n",
       "1457          1  \n",
       "1458          1  \n",
       "1459          1  \n",
       "1460          1  \n",
       "1461          1  \n",
       "\n",
       "[1462 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d0e712a-7587-4aee-a9f9-469e4ac3c745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode de la colonne 'industry' : retail\n",
      "\n",
      "Fréquence des valeurs les plus courantes :\n",
      "industry\n",
      "retail        203\n",
      "finance       200\n",
      "other         198\n",
      "healthcare    187\n",
      "education     187\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df['industry'] = df['industry'].fillna('NA')\n",
    "\n",
    "# Trouver le mode (valeur la plus fréquente)\n",
    "mode_industry = df['industry'].mode()[0]\n",
    "\n",
    "print(\"Mode de la colonne 'industry' :\", mode_industry)\n",
    "\n",
    "# Pour vérifier la fréquence\n",
    "print(\"\\nFréquence des valeurs les plus courantes :\")\n",
    "print(df['industry'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cebd58b-48b3-4d47-9574-70cbeb4297d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de corrélation :\n",
      "\n",
      "                          number_of_courses_viewed  annual_income  \\\n",
      "number_of_courses_viewed                  1.000000       0.009770   \n",
      "annual_income                             0.009770       1.000000   \n",
      "interaction_count                        -0.023565       0.027036   \n",
      "lead_score                               -0.004879       0.015610   \n",
      "\n",
      "                          interaction_count  lead_score  \n",
      "number_of_courses_viewed          -0.023565   -0.004879  \n",
      "annual_income                      0.027036    0.015610  \n",
      "interaction_count                  1.000000    0.009888  \n",
      "lead_score                         0.009888    1.000000   \n",
      "\n",
      "interaction_count & lead_score: 0.010\n",
      "number_of_courses_viewed & lead_score: -0.005\n",
      "number_of_courses_viewed & interaction_count: -0.024\n",
      "annual_income & interaction_count: 0.027\n",
      "\n",
      "✅ La plus grande corrélation est entre : annual_income & interaction_count\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Remplacer les valeurs manquantes\n",
    "categorical = ['lead_source', 'industry', 'employment_status', 'location']\n",
    "numerical = ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
    "\n",
    "df[categorical] = df[categorical].fillna('NA')\n",
    "df[numerical] = df[numerical].fillna(0.0)\n",
    "\n",
    "# Sélection des variables numériques\n",
    "df_num = df[numerical]\n",
    "\n",
    "# Matrice de corrélation\n",
    "corr_matrix = df_num.corr()\n",
    "\n",
    "print(\"Matrice de corrélation :\\n\")\n",
    "print(corr_matrix, \"\\n\")\n",
    "\n",
    "# Extraire les corrélations demandées :\n",
    "pairs = {\n",
    "    \"interaction_count & lead_score\": corr_matrix.loc['interaction_count', 'lead_score'],\n",
    "    \"number_of_courses_viewed & lead_score\": corr_matrix.loc['number_of_courses_viewed', 'lead_score'],\n",
    "    \"number_of_courses_viewed & interaction_count\": corr_matrix.loc['number_of_courses_viewed', 'interaction_count'],\n",
    "    \"annual_income & interaction_count\": corr_matrix.loc['annual_income', 'interaction_count']\n",
    "}\n",
    "\n",
    "# Afficher les corrélations des paires demandées\n",
    "for k, v in pairs.items():\n",
    "    print(f\"{k}: {v:.3f}\")\n",
    "\n",
    "# Identifier la paire avec la corrélation la plus élevée\n",
    "max_pair = max(pairs, key=pairs.get)\n",
    "print(f\"\\n✅ La plus grande corrélation est entre : {max_pair}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0156a70-7c3f-45fb-8d43-5d14947f67c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores d'information mutuelle :\n",
      "\n",
      "lead_source          0.04\n",
      "employment_status    0.01\n",
      "industry             0.01\n",
      "location             0.00\n",
      "dtype: float64\n",
      "\n",
      "✅ Variable avec le score le plus élevé : lead_source\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Charger le dataset\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\")\n",
    "\n",
    "# Colonnes catégorielles et numériques\n",
    "categorical = ['lead_source', 'industry', 'employment_status', 'location']\n",
    "numerical = ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
    "\n",
    "# Remplacement des valeurs manquantes\n",
    "df[categorical] = df[categorical].fillna('NA')\n",
    "df[numerical] = df[numerical].fillna(0.0)\n",
    "\n",
    "# Séparer la cible\n",
    "y = df['converted']\n",
    "X = df.drop('converted', axis=1)\n",
    "\n",
    "# Division en train / test (temporaire)\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Division du train en train / val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.25, random_state=42)\n",
    "# (0.25 de 0.8 = 0.2, donc 60 / 20 / 20)\n",
    "\n",
    "# Encoder les variables catégorielles (transformer en catégories numériques)\n",
    "X_train_encoded = X_train[categorical].apply(lambda col: col.astype('category').cat.codes)\n",
    "\n",
    "# Calculer le score d’information mutuelle\n",
    "mi_scores = mutual_info_classif(X_train_encoded, y_train, discrete_features=True, random_state=42)\n",
    "\n",
    "# Créer un DataFrame pour les résultats\n",
    "mi_results = pd.Series(mi_scores, index=categorical).sort_values(ascending=False)\n",
    "mi_results = mi_results.round(2)\n",
    "\n",
    "print(\"Scores d'information mutuelle :\\n\")\n",
    "print(mi_results)\n",
    "print(\"\\n✅ Variable avec le score le plus élevé :\", mi_results.idxmax())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6430b622-c29d-4b95-8d92-b848ea62724b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (1169, 8)\n",
      "Validation set: (293, 8)\n",
      "\n",
      "=== RÉSULTATS ===\n",
      "Précision sur l'ensemble de validation: 0.8328\n",
      "Précision arrondie à 2 décimales: 0.83\n",
      "\n",
      "=== OPTIONS ===\n",
      "0.64 → ✗\n",
      "0.74 → ✗\n",
      "0.84 → ✗\n",
      "0.94 → ✗\n",
      "\n",
      "=== INFORMATIONS SUPPLÉMENTAIRES ===\n",
      "Taille de l'ensemble d'entraînement: 1169\n",
      "Taille de l'ensemble de validation: 293\n",
      "Proportion de la classe positive (converted=1): 61.90%\n",
      "\n",
      "Variables catégorielles après one-hot encoding:\n",
      "Dimensions après one-hot encoding: (1169, 27)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Chargement des données\n",
    "url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# 1. Traitement des valeurs manquantes selon les instructions\n",
    "categorical_features = ['lead_source', 'industry', 'employment_status', 'location']\n",
    "numerical_features = ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
    "\n",
    "# Variables catégorielles → \"NA\"\n",
    "for col in categorical_features:\n",
    "    df[col] = df[col].fillna('NA')\n",
    "\n",
    "# Variables numériques → 0.0\n",
    "for col in numerical_features:\n",
    "    df[col] = df[col].fillna(0.0)\n",
    "\n",
    "# 2. Séparation des features et de la target\n",
    "X = df.drop('converted', axis=1)\n",
    "y = df['converted']\n",
    "\n",
    "# 3. Split train/validation (généralement 80/20 ou 70/30)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Train set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "\n",
    "# 4. Préprocessing avec One-Hot Encoding pour les variables catégorielles\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# 5. Pipeline avec preprocessing et modèle Logistic Regression\n",
    "model = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# 6. Entraînement du modèle\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 7. Prédiction sur l'ensemble de validation\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# 8. Calcul de la précision\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "accuracy_rounded = round(accuracy, 2)\n",
    "\n",
    "print(f\"\\n=== RÉSULTATS ===\")\n",
    "print(f\"Précision sur l'ensemble de validation: {accuracy:.4f}\")\n",
    "print(f\"Précision arrondie à 2 décimales: {accuracy_rounded}\")\n",
    "\n",
    "# 9. Vérification des options de réponse\n",
    "print(f\"\\n=== OPTIONS ===\")\n",
    "print(f\"0.64 → {'✓' if accuracy_rounded == 0.64 else '✗'}\")\n",
    "print(f\"0.74 → {'✓' if accuracy_rounded == 0.74 else '✗'}\")\n",
    "print(f\"0.84 → {'✓' if accuracy_rounded == 0.84 else '✗'}\")\n",
    "print(f\"0.94 → {'✓' if accuracy_rounded == 0.94 else '✗'}\")\n",
    "\n",
    "# 10. Informations supplémentaires sur les données\n",
    "print(f\"\\n=== INFORMATIONS SUPPLÉMENTAIRES ===\")\n",
    "print(f\"Taille de l'ensemble d'entraînement: {len(X_train)}\")\n",
    "print(f\"Taille de l'ensemble de validation: {len(X_val)}\")\n",
    "print(f\"Proportion de la classe positive (converted=1): {y.mean():.2%}\")\n",
    "\n",
    "# Vérification du preprocessing\n",
    "print(f\"\\nVariables catégorielles après one-hot encoding:\")\n",
    "preprocessor.fit(X_train)\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "print(f\"Dimensions après one-hot encoding: {X_train_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8182e50e-3c26-4a7d-ba24-2ee4dca61238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Précision avec toutes les features: 0.832765\n",
      "\n",
      "=== ANALYSE PAR ÉLIMINATION DE FEATURES ===\n",
      "Sans 'industry':\n",
      "  Précision: 0.819113\n",
      "  Différence: 0.013652\n",
      "Sans 'employment_status':\n",
      "  Précision: 0.805461\n",
      "  Différence: 0.027304\n",
      "Sans 'lead_score':\n",
      "  Précision: 0.822526\n",
      "  Différence: 0.010239\n",
      "\n",
      "=== RÉSULTAT ===\n",
      "La feature avec la plus petite différence est: 'lead_score'\n",
      "Avec une différence de: 0.010239\n",
      "\n",
      "=== COMPARAISON DÉTAILLÉE ===\n",
      "'lead_score': différence = 0.010239\n",
      "'industry': différence = 0.013652\n",
      "'employment_status': différence = 0.027304\n",
      "\n",
      "=== VÉRIFICATION DES OPTIONS ===\n",
      "'industry' → différence = 0.013652\n",
      "'employment_status' → différence = 0.027304\n",
      "'lead_score' → différence = 0.010239\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Chargement des données\n",
    "url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Traitement des valeurs manquantes\n",
    "categorical_features = ['lead_source', 'industry', 'employment_status', 'location']\n",
    "numerical_features = ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
    "\n",
    "for col in categorical_features:\n",
    "    df[col] = df[col].fillna('NA')\n",
    "for col in numerical_features:\n",
    "    df[col] = df[col].fillna(0.0)\n",
    "\n",
    "X = df.drop('converted', axis=1)\n",
    "y = df['converted']\n",
    "\n",
    "# Split train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Fonction pour créer le modèle avec un ensemble de features spécifique\n",
    "def train_model(features_to_use):\n",
    "    # Identifier les types de features pour le sous-ensemble\n",
    "    cat_subset = [f for f in categorical_features if f in features_to_use]\n",
    "    num_subset = [f for f in numerical_features if f in features_to_use]\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), num_subset),\n",
    "            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_subset)\n",
    "        ])\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(solver='liblinear', C=1.0, max_iter=1000, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train[features_to_use], y_train)\n",
    "    y_pred = model.predict(X_val[features_to_use])\n",
    "    return accuracy_score(y_val, y_pred)\n",
    "\n",
    "# Précision avec toutes les features\n",
    "all_features = categorical_features + numerical_features\n",
    "original_accuracy = train_model(all_features)\n",
    "print(f\"Précision avec toutes les features: {original_accuracy:.6f}\")\n",
    "\n",
    "# Test en excluant chaque feature une par une\n",
    "feature_differences = {}\n",
    "\n",
    "print(\"\\n=== ANALYSE PAR ÉLIMINATION DE FEATURES ===\")\n",
    "for feature in ['industry', 'employment_status', 'lead_score']:\n",
    "    features_without = [f for f in all_features if f != feature]\n",
    "    accuracy_without = train_model(features_without)\n",
    "    difference = original_accuracy - accuracy_without\n",
    "    feature_differences[feature] = difference\n",
    "    \n",
    "    print(f\"Sans '{feature}':\")\n",
    "    print(f\"  Précision: {accuracy_without:.6f}\")\n",
    "    print(f\"  Différence: {difference:.6f}\")\n",
    "\n",
    "# Trouver la feature avec la plus petite différence\n",
    "min_diff_feature = min(feature_differences, key=feature_differences.get)\n",
    "min_diff_value = feature_differences[min_diff_feature]\n",
    "\n",
    "print(f\"\\n=== RÉSULTAT ===\")\n",
    "print(f\"La feature avec la plus petite différence est: '{min_diff_feature}'\")\n",
    "print(f\"Avec une différence de: {min_diff_value:.6f}\")\n",
    "\n",
    "# Affichage détaillé pour vérification\n",
    "print(\"\\n=== COMPARAISON DÉTAILLÉE ===\")\n",
    "for feature, diff in sorted(feature_differences.items(), key=lambda x: abs(x[1])):\n",
    "    print(f\"'{feature}': différence = {diff:.6f}\")\n",
    "\n",
    "# Vérification des options\n",
    "print(f\"\\n=== VÉRIFICATION DES OPTIONS ===\")\n",
    "print(f\"'industry' → différence = {feature_differences['industry']:.6f}\")\n",
    "print(f\"'employment_status' → différence = {feature_differences['employment_status']:.6f}\")\n",
    "print(f\"'lead_score' → différence = {feature_differences['lead_score']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a83733fc-a63f-4510-9127-1515dd0c356b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RÉGRESSION LOGISTIQUE RÉGULARISÉE ===\n",
      "Test des différentes valeurs de C:\n",
      "\n",
      "C =  0.01: Précision = 0.815700 → Arrondi = 0.816\n",
      "C =   0.1: Précision = 0.815700 → Arrondi = 0.816\n",
      "C =     1: Précision = 0.832765 → Arrondi = 0.833\n",
      "C =    10: Précision = 0.832765 → Arrondi = 0.833\n",
      "C =   100: Précision = 0.839590 → Arrondi = 0.84\n",
      "\n",
      "=== MEILLEUR RÉSULTAT ===\n",
      "Meilleur C: 100\n",
      "Meilleure précision: 0.839590\n",
      "Meilleure précision arrondie: 0.84\n",
      "\n",
      "=== VÉRIFICATION DES OPTIONS ===\n",
      "C =  0.01 → 0.816 ✗\n",
      "C =   0.1 → 0.816 ✗\n",
      "C =     1 → 0.833 ✗\n",
      "C =    10 → 0.833 ✗\n",
      "C =   100 → 0.84 ✓\n",
      "\n",
      "=== ANALYSE DE LA RÉGULARISATION ===\n",
      "C plus petit = plus de régularisation (pénalité plus forte)\n",
      "C plus grand = moins de régularisation (pénalité plus faible)\n",
      "\n",
      "=== CONVERGENCE ===\n",
      "C =  0.01: 4 itérations\n",
      "C =   0.1: 5 itérations\n",
      "C =     1: 6 itérations\n",
      "C =    10: 6 itérations\n",
      "C =   100: 6 itérations\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Chargement des données\n",
    "url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Traitement des valeurs manquantes\n",
    "categorical_features = ['lead_source', 'industry', 'employment_status', 'location']\n",
    "numerical_features = ['number_of_courses_viewed', 'annual_income', 'interaction_count', 'lead_score']\n",
    "\n",
    "for col in categorical_features:\n",
    "    df[col] = df[col].fillna('NA')\n",
    "for col in numerical_features:\n",
    "    df[col] = df[col].fillna(0.0)\n",
    "\n",
    "X = df.drop('converted', axis=1)\n",
    "y = df['converted']\n",
    "\n",
    "# Split train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Préprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Valeurs de C à tester\n",
    "C_values = [0.01, 0.1, 1, 10, 100]\n",
    "results = []\n",
    "\n",
    "print(\"=== RÉGRESSION LOGISTIQUE RÉGULARISÉE ===\")\n",
    "print(\"Test des différentes valeurs de C:\\n\")\n",
    "\n",
    "best_accuracy = 0\n",
    "best_C = None\n",
    "\n",
    "for C in C_values:\n",
    "    # Modèle avec la valeur de C actuelle\n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Entraînement et prédiction\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calcul de la précision\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    accuracy_rounded = round(accuracy, 3)\n",
    "    \n",
    "    results.append({\n",
    "        'C': C,\n",
    "        'accuracy': accuracy,\n",
    "        'accuracy_rounded': accuracy_rounded\n",
    "    })\n",
    "    \n",
    "    print(f\"C = {C:5}: Précision = {accuracy:.6f} → Arrondi = {accuracy_rounded}\")\n",
    "    \n",
    "    # Mise à jour de la meilleure valeur\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_C = C\n",
    "        best_accuracy_rounded = accuracy_rounded\n",
    "\n",
    "print(f\"\\n=== MEILLEUR RÉSULTAT ===\")\n",
    "print(f\"Meilleur C: {best_C}\")\n",
    "print(f\"Meilleure précision: {best_accuracy:.6f}\")\n",
    "print(f\"Meilleure précision arrondie: {best_accuracy_rounded}\")\n",
    "\n",
    "# Affichage des résultats pour chaque option\n",
    "print(f\"\\n=== VÉRIFICATION DES OPTIONS ===\")\n",
    "for result in results:\n",
    "    is_best = \"✓\" if result['C'] == best_C else \"✗\"\n",
    "    print(f\"C = {result['C']:5} → {result['accuracy_rounded']} {is_best}\")\n",
    "\n",
    "# Analyse supplémentaire : impact de la régularisation\n",
    "print(f\"\\n=== ANALYSE DE LA RÉGULARISATION ===\")\n",
    "print(\"C plus petit = plus de régularisation (pénalité plus forte)\")\n",
    "print(\"C plus grand = moins de régularisation (pénalité plus faible)\")\n",
    "\n",
    "# Vérification du nombre d'itérations nécessaires\n",
    "print(f\"\\n=== CONVERGENCE ===\")\n",
    "for C in C_values:\n",
    "    model_test = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(solver='liblinear', C=C, max_iter=1000, random_state=42))\n",
    "    ])\n",
    "    model_test.fit(X_train, y_train)\n",
    "    n_iter = model_test.named_steps['classifier'].n_iter_[0]\n",
    "    print(f\"C = {C:5}: {n_iter} itérations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4799f445-0b6c-4aad-9580-ae33faea5184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
